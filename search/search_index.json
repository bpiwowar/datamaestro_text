{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This datasets plugin covers text-related datasets: Information Retrieval Natural Language Processing tasks","title":"Home"},{"location":"datasets/text/io.github.rajpurkar.squad.html","text":"io.github.rajpurkar.squad Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD [None]","title":"SQuAD"},{"location":"datasets/text/io.github.rajpurkar.squad.html#iogithubrajpurkarsquad","text":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD [None]","title":"io.github.rajpurkar.squad"},{"location":"datasets/text/ai.quac.html","text":"ai.quac Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context. None [None]","title":"ai.quac"},{"location":"datasets/text/ai.quac.html#aiquac","text":"Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context. None [None]","title":"ai.quac"},{"location":"datasets/text/gov.nist.trec.index.html","text":"gov.nist.trec.index The Text REtrieval Conference (TREC), co-sponsored by the National Institute of Standards and Technology (NIST) and U.S. Department of Defense, was started in 1992 as part of the TIPSTER Text program. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In particular, the TREC workshop series has the following goals: to encourage research in information retrieval based on large test collections; to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provides a test set of documents and questions. Participants run their own retrieval systems on the data, and return to NIST a list of the retrieved top-ranked documents. NIST pools the individual results, judges the retrieved documents for correctness, and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. This evaluation effort has grown in both the number of participating systems and the number of tasks each year. Ninety-three groups representing 22 countries participated in TREC 2003. The TREC test collections and evaluation software are available to the retrieval research community at large, so organizations can evaluate their own retrieval systems at any time. TREC has successfully met its dual goals of improving the state-of-the-art in information retrieval and of facilitating technology transfer. Retrieval system effectiveness approximately doubled in the first six years of TREC. TREC has also sponsored the first large-scale evaluations of the retrieval of non-English (Spanish and Chinese) documents, retrieval of recordings of speech, and retrieval across multiple languages. TREC has also introduced evaluations for open-domain question answering and content-based retrieval of digital video. The TREC test collections are large enough so that they realistically model operational settings. Most of today's commercial search engines include technology first developed in TREC. gov.nist.trec.index [None]","title":"gov.nist.trec.index"},{"location":"datasets/text/gov.nist.trec.index.html#govnisttrecindex","text":"The Text REtrieval Conference (TREC), co-sponsored by the National Institute of Standards and Technology (NIST) and U.S. Department of Defense, was started in 1992 as part of the TIPSTER Text program. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In particular, the TREC workshop series has the following goals: to encourage research in information retrieval based on large test collections; to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provides a test set of documents and questions. Participants run their own retrieval systems on the data, and return to NIST a list of the retrieved top-ranked documents. NIST pools the individual results, judges the retrieved documents for correctness, and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. This evaluation effort has grown in both the number of participating systems and the number of tasks each year. Ninety-three groups representing 22 countries participated in TREC 2003. The TREC test collections and evaluation software are available to the retrieval research community at large, so organizations can evaluate their own retrieval systems at any time. TREC has successfully met its dual goals of improving the state-of-the-art in information retrieval and of facilitating technology transfer. Retrieval system effectiveness approximately doubled in the first six years of TREC. TREC has also sponsored the first large-scale evaluations of the retrieval of non-English (Spanish and Chinese) documents, retrieval of recordings of speech, and retrieval across multiple languages. TREC has also introduced evaluations for open-domain question answering and content-based retrieval of digital video. The TREC test collections are large enough so that they realistically model operational settings. Most of today's commercial search engines include technology first developed in TREC. gov.nist.trec.index [None]","title":"gov.nist.trec.index"},{"location":"datasets/text/gov.nist.trec.web.html","text":"gov.nist.trec.web gov.nist.trec.web.2009.adhoc [trec.adhoc/task]","title":"gov.nist.trec.web"},{"location":"datasets/text/gov.nist.trec.web.html#govnisttrecweb","text":"gov.nist.trec.web.2009.adhoc [trec.adhoc/task]","title":"gov.nist.trec.web"},{"location":"datasets/text/gov.nist.trec.adhoc.html","text":"gov.nist.trec.adhoc TREC adhoc datasets TREC-1 to TREC-3 documents [documents/collection] TREC 1 topics [trec.adhoc/topics] TREC 1 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 1 (1992) [trec.adhoc/task] TREC 2 topics [trec.adhoc/topics] TREC 2 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 2 (1993) [trec.adhoc/task] TREC 3 topics [trec.adhoc/topics] TREC 3 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 3 (1994) [trec.adhoc/task] Data collection used in TREC-4 [documents/collection] TREC 4 topics [trec.adhoc/topics] TREC 4 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 4 (1995) [trec.adhoc/task] Data collection used in TREC-5 [documents/collection] TREC 5 topics [trec.adhoc/topics] TREC 5 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 5 (1996) [trec.adhoc/task] Data collection used in TREC-6 [documents/collection] TREC 6 topics [trec.adhoc/topics] TREC 6 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 6 (1997) [trec.adhoc/task] Data collection used in TREC-7 and TREC-8 [documents/collection] TREC 7 topics [trec.adhoc/topics] TREC 7 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 3 (1994) [trec.adhoc/task] TREC 8 topics [trec.adhoc/topics] TREC 8 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 8 (1999) [trec.adhoc/task] TREC Robust 2004 topics [trec.adhoc/topics] TREC Robust 2004 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC Robust (2004) [trec.adhoc/task] TREC Robust 2005 topics [trec.adhoc/topics] TREC Robust 2005 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC Robust (2005) [trec.adhoc/task]","title":"TREC adhoc datasets"},{"location":"datasets/text/gov.nist.trec.adhoc.html#govnisttrecadhoc","text":"TREC adhoc datasets TREC-1 to TREC-3 documents [documents/collection] TREC 1 topics [trec.adhoc/topics] TREC 1 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 1 (1992) [trec.adhoc/task] TREC 2 topics [trec.adhoc/topics] TREC 2 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 2 (1993) [trec.adhoc/task] TREC 3 topics [trec.adhoc/topics] TREC 3 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 3 (1994) [trec.adhoc/task] Data collection used in TREC-4 [documents/collection] TREC 4 topics [trec.adhoc/topics] TREC 4 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 4 (1995) [trec.adhoc/task] Data collection used in TREC-5 [documents/collection] TREC 5 topics [trec.adhoc/topics] TREC 5 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 5 (1996) [trec.adhoc/task] Data collection used in TREC-6 [documents/collection] TREC 6 topics [trec.adhoc/topics] TREC 6 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 6 (1997) [trec.adhoc/task] Data collection used in TREC-7 and TREC-8 [documents/collection] TREC 7 topics [trec.adhoc/topics] TREC 7 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 3 (1994) [trec.adhoc/task] TREC 8 topics [trec.adhoc/topics] TREC 8 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC 8 (1999) [trec.adhoc/task] TREC Robust 2004 topics [trec.adhoc/topics] TREC Robust 2004 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC Robust (2004) [trec.adhoc/task] TREC Robust 2005 topics [trec.adhoc/topics] TREC Robust 2005 relevance assessments [trec.adhoc/assessments] Ad-hoc task of TREC Robust (2005) [trec.adhoc/task]","title":"gov.nist.trec.adhoc"},{"location":"datasets/text/gov.nist.trec.clueweb.html","text":"gov.nist.trec.clueweb gov.nist.trec.clueweb.09 [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.09b [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.12 [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.12b [{'documents/list': {'type': 'warc'}}]","title":"gov.nist.trec.clueweb"},{"location":"datasets/text/gov.nist.trec.clueweb.html#govnisttrecclueweb","text":"gov.nist.trec.clueweb.09 [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.09b [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.12 [{'documents/list': {'type': 'warc'}}] gov.nist.trec.clueweb.12b [{'documents/list': {'type': 'warc'}}]","title":"gov.nist.trec.clueweb"},{"location":"datasets/text/gov.nist.trec.tipster.html","text":"gov.nist.trec.tipster TIPSTER is sometimes also called the Text Research Collection Volume or TREC. The TIPSTER project was sponsored by the Software and Intelligent Systems Technology Office of the Advanced Research Projects Agency (ARPA/SISTO) in an effort to significantly advance the state of the art in effective document detection (information retrieval) and data extraction from large, real-world data collections. The detection data is comprised of a test collection built at NIST for the TIPSTER project and the related TREC project. The TREC project has many other participating information retrieval research groups, working on the same task as the TIPSTER groups, but meeting once a year in a workshop to compare results (similar to MUC). The test collection consists of three CD-ROMs of SGML encoded documents distributed by LDC plus queries and answers (relevant documents) distributed by NIST. gov.nist.trec.tipster.ap88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ap89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ap90 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.doe1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj87 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj90 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj91 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj92 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr94 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff2 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff3 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.sjm1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.cr1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ft1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fbis1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.la8990 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}]","title":"gov.nist.trec.tipster"},{"location":"datasets/text/gov.nist.trec.tipster.html#govnisttrectipster","text":"TIPSTER is sometimes also called the Text Research Collection Volume or TREC. The TIPSTER project was sponsored by the Software and Intelligent Systems Technology Office of the Advanced Research Projects Agency (ARPA/SISTO) in an effort to significantly advance the state of the art in effective document detection (information retrieval) and data extraction from large, real-world data collections. The detection data is comprised of a test collection built at NIST for the TIPSTER project and the related TREC project. The TREC project has many other participating information retrieval research groups, working on the same task as the TIPSTER groups, but meeting once a year in a workshop to compare results (similar to MUC). The test collection consists of three CD-ROMs of SGML encoded documents distributed by LDC plus queries and answers (relevant documents) distributed by NIST. gov.nist.trec.tipster.ap88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ap89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ap90 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.doe1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj87 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj90 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj91 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.wsj92 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr88 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr89 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fr94 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff2 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ziff3 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.sjm1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.cr1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.ft1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.fbis1 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}] gov.nist.trec.tipster.la8990 [{'documents/list': {'type': 'ds.gov.nist.tipster'}}]","title":"gov.nist.trec.tipster"},{"location":"datasets/text/edu.upenn.ldc.nyt.html","text":"edu.upenn.ldc.nyt edu.upenn.ldc.nyt [{'documents/list': {'type': 'org.iptc.nift'}}]","title":"edu.upenn.ldc.nyt"},{"location":"datasets/text/edu.upenn.ldc.nyt.html#eduupennldcnyt","text":"edu.upenn.ldc.nyt [{'documents/list': {'type': 'org.iptc.nift'}}]","title":"edu.upenn.ldc.nyt"},{"location":"datasets/text/edu.upenn.ldc.aquaint.html","text":"edu.upenn.ldc.aquaint edu.upenn.ldc.aquaint.apt [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint.nyt [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint.xie [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint [documents/collection]","title":"edu.upenn.ldc.aquaint"},{"location":"datasets/text/edu.upenn.ldc.aquaint.html#eduupennldcaquaint","text":"edu.upenn.ldc.aquaint.apt [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint.nyt [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint.xie [{'documents/list': {'type': 'gov.nist.tipster.sgml'}}] edu.upenn.ldc.aquaint [documents/collection]","title":"edu.upenn.ldc.aquaint"},{"location":"datasets/text/edu.cornell.nlvr.html","text":"edu.cornell.nlvr Cornell Natural Language Visual Reasoning (NLVR) is a language grounding dataset. It contains 92,244 pairs of natural language statements grounded in synthetic images. The task is to determine whether a sentence is true or false about an image. The data was collected through crowdsourcing, and requires reasoning about sets of objects, quantities, comparisons, and spatial relations. Cornell Natural Language Visual Reasoning [None]","title":"Cornell Natural Language Visual Reasoning"},{"location":"datasets/text/edu.cornell.nlvr.html#educornellnlvr","text":"Cornell Natural Language Visual Reasoning (NLVR) is a language grounding dataset. It contains 92,244 pairs of natural language statements grounded in synthetic images. The task is to determine whether a sentence is true or false about an image. The data was collected through crowdsourcing, and requires reasoning about sets of objects, quantities, comparisons, and spatial relations. Cornell Natural Language Visual Reasoning [None]","title":"edu.cornell.nlvr"},{"location":"datasets/text/edu.standford.im2p.html","text":"edu.standford.im2p Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach. With this paper, we are releasing a new dataset to allow researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2487/2489 images. We show in our paper that the paragraphs are more diverse than their corresponding sentences descriptions with more verbs, co-references and adjectives. Since all the images are also part of the Visual Genome dataset, Each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs. ![https://cs.stanford.edu/people/ranjaykrishna/im2p/examples.png] IM2P [None]","title":"IM2P"},{"location":"datasets/text/edu.standford.im2p.html#edustandfordim2p","text":"Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach. With this paper, we are releasing a new dataset to allow researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2487/2489 images. We show in our paper that the paragraphs are more diverse than their corresponding sentences descriptions with more verbs, co-references and adjectives. Since all the images are also part of the Visual Genome dataset, Each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs. ![https://cs.stanford.edu/people/ranjaykrishna/im2p/examples.png] IM2P [None]","title":"edu.standford.im2p"},{"location":"datasets/text/edu.standford.glove.html","text":"edu.standford.glove GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. GloVe word embeddings [None] GloVe word embeddings [None] GloVe word embeddings [None]","title":"GloVe word embeddings"},{"location":"datasets/text/edu.standford.glove.html#edustandfordglove","text":"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. GloVe word embeddings [None] GloVe word embeddings [None] GloVe word embeddings [None]","title":"edu.standford.glove"},{"location":"datasets/text/uk.ac.ucl.cs.qangaroo.html","text":"uk.ac.ucl.cs.qangaroo We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. Several pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps. Our aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents. The two QAngaroo datasets provide a training and evaluation resource for such methods. QAngaroo [None]","title":"QAngaroo"},{"location":"datasets/text/uk.ac.ucl.cs.qangaroo.html#ukacuclcsqangaroo","text":"We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. Several pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps. Our aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents. The two QAngaroo datasets provide a training and evaluation resource for such methods. QAngaroo [None]","title":"uk.ac.ucl.cs.qangaroo"},{"location":"datasets/text/org.grouplens.movielens20m.html","text":"org.grouplens.movielens20m org.grouplens.movielens20m [None]","title":"org.grouplens.movielens20m"},{"location":"datasets/text/org.grouplens.movielens20m.html#orggrouplensmovielens20m","text":"org.grouplens.movielens20m [None]","title":"org.grouplens.movielens20m"},{"location":"datasets/text/org.acm.recsys.cb2014.html","text":"org.acm.recsys.cb2014 org.acm.recsys.cb2014 [None]","title":"org.acm.recsys.cb2014"},{"location":"datasets/text/org.acm.recsys.cb2014.html#orgacmrecsyscb2014","text":"org.acm.recsys.cb2014 [None]","title":"org.acm.recsys.cb2014"},{"location":"datasets/text/org.visualqa.balanced.html","text":"org.visualqa.balanced Balanced Real Images [None] Balanced Abstract Images [None]","title":"Visual Question Answering v2"},{"location":"datasets/text/org.visualqa.balanced.html#orgvisualqabalanced","text":"Balanced Real Images [None] Balanced Abstract Images [None]","title":"org.visualqa.balanced"},{"location":"datasets/text/org.cocodataset.index.html","text":"org.cocodataset.index COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoints COmmon Objects in Context [None]","title":"COmmon Objects in Context"},{"location":"datasets/text/org.cocodataset.index.html#orgcocodatasetindex","text":"COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoints COmmon Objects in Context [None]","title":"org.cocodataset.index"}]}