{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This datamaestro plugin covers text-related datasets: Information Retrieval Natural Language Processing tasks Recommendation","title":"Home"},{"location":"datamaestro/tasks.html","text":"List of Tasks collaborative filtering dependency parsing information extraction language model language modeling POS parsing Question Answering recommendation Visual Question Answering","title":"List of Tasks"},{"location":"datamaestro/tasks.html#list-of-tasks","text":"collaborative filtering dependency parsing information extraction language model language modeling POS parsing Question Answering recommendation Visual Question Answering","title":"List of Tasks"},{"location":"datamaestro/tags.html","text":"List of Tags caption collaborative filtering Context Dialogue documents french html image language comprehension multi-hop natural language inference novels object recognition paragraphs question anwering recommendation segmented images synthetic text visual inference visual reasoning web word embeddings","title":"List of tags"},{"location":"datamaestro/tags.html#list-of-tags","text":"caption collaborative filtering Context Dialogue documents french html image language comprehension multi-hop natural language inference novels object recognition paragraphs question anwering recommendation segmented images synthetic text visual inference visual reasoning web word embeddings","title":"List of Tags"},{"location":"datamaestro/tags/documents.html","text":"documents gov.nist.trec.clueweb gov.nist.trec.web","title":"documents"},{"location":"datamaestro/tags/documents.html#documents","text":"gov.nist.trec.clueweb gov.nist.trec.web","title":"documents"},{"location":"datamaestro/tags/text.html","text":"text gov.nist.trec.clueweb gov.nist.trec.web BookCorpus Wiki-2 Wiki-103","title":"text"},{"location":"datamaestro/tags/text.html#text","text":"gov.nist.trec.clueweb gov.nist.trec.web BookCorpus Wiki-2 Wiki-103","title":"text"},{"location":"datamaestro/tags/html.html","text":"html gov.nist.trec.clueweb gov.nist.trec.web","title":"html"},{"location":"datamaestro/tags/html.html#html","text":"gov.nist.trec.clueweb gov.nist.trec.web","title":"html"},{"location":"datamaestro/tags/web.html","text":"web gov.nist.trec.clueweb gov.nist.trec.web","title":"web"},{"location":"datamaestro/tags/web.html#web","text":"gov.nist.trec.clueweb gov.nist.trec.web","title":"web"},{"location":"datamaestro/tags/synthetic.html","text":"synthetic bAbI","title":"synthetic"},{"location":"datamaestro/tags/synthetic.html#synthetic","text":"bAbI","title":"synthetic"},{"location":"datamaestro/tags/novels.html","text":"novels BookCorpus","title":"novels"},{"location":"datamaestro/tags/novels.html#novels","text":"BookCorpus","title":"novels"},{"location":"datamaestro/tags/language comprehension.html","text":"language comprehension SQuAD QAngaroo Large Text Compression Benchmark","title":"language comprehension"},{"location":"datamaestro/tags/language comprehension.html#language-comprehension","text":"SQuAD QAngaroo Large Text Compression Benchmark","title":"language comprehension"},{"location":"datamaestro/tags/natural language inference.html","text":"natural language inference SQuAD QAngaroo Large Text Compression Benchmark","title":"natural language inference"},{"location":"datamaestro/tags/natural language inference.html#natural-language-inference","text":"SQuAD QAngaroo Large Text Compression Benchmark","title":"natural language inference"},{"location":"datamaestro/tags/question anwering.html","text":"question anwering SQuAD Large Text Compression Benchmark","title":"question anwering"},{"location":"datamaestro/tags/question anwering.html#question-anwering","text":"SQuAD Large Text Compression Benchmark","title":"question anwering"},{"location":"datamaestro/tags/context.html","text":"Context QUAC","title":"Context"},{"location":"datamaestro/tags/context.html#context","text":"QUAC","title":"Context"},{"location":"datamaestro/tags/dialogue.html","text":"Dialogue QUAC","title":"Dialogue"},{"location":"datamaestro/tags/dialogue.html#dialogue","text":"QUAC","title":"Dialogue"},{"location":"datamaestro/tags/collaborative filtering.html","text":"collaborative filtering CBRecSys 2014","title":"collaborative filtering"},{"location":"datamaestro/tags/collaborative filtering.html#collaborative-filtering","text":"CBRecSys 2014","title":"collaborative filtering"},{"location":"datamaestro/tags/recommendation.html","text":"recommendation CBRecSys 2014","title":"recommendation"},{"location":"datamaestro/tags/recommendation.html#recommendation","text":"CBRecSys 2014","title":"recommendation"},{"location":"datamaestro/tags/image.html","text":"image COmmon Objects in Context IM2P","title":"image"},{"location":"datamaestro/tags/image.html#image","text":"COmmon Objects in Context IM2P","title":"image"},{"location":"datamaestro/tags/segmented images.html","text":"segmented images COmmon Objects in Context","title":"segmented images"},{"location":"datamaestro/tags/segmented images.html#segmented-images","text":"COmmon Objects in Context","title":"segmented images"},{"location":"datamaestro/tags/object recognition.html","text":"object recognition COmmon Objects in Context","title":"object recognition"},{"location":"datamaestro/tags/object recognition.html#object-recognition","text":"COmmon Objects in Context","title":"object recognition"},{"location":"datamaestro/tags/french.html","text":"french UD_French-ParTUT UD_French-ParTUT","title":"french"},{"location":"datamaestro/tags/french.html#french","text":"UD_French-ParTUT UD_French-ParTUT","title":"french"},{"location":"datamaestro/tags/multi-hop.html","text":"multi-hop QAngaroo","title":"multi-hop"},{"location":"datamaestro/tags/multi-hop.html#multi-hop","text":"QAngaroo","title":"multi-hop"},{"location":"datamaestro/tags/visual reasoning.html","text":"visual reasoning Cornell Natural Language Visual Reasoning","title":"visual reasoning"},{"location":"datamaestro/tags/visual reasoning.html#visual-reasoning","text":"Cornell Natural Language Visual Reasoning","title":"visual reasoning"},{"location":"datamaestro/tags/visual inference.html","text":"visual inference Cornell Natural Language Visual Reasoning","title":"visual inference"},{"location":"datamaestro/tags/visual inference.html#visual-inference","text":"Cornell Natural Language Visual Reasoning","title":"visual inference"},{"location":"datamaestro/tags/caption.html","text":"caption IM2P","title":"caption"},{"location":"datamaestro/tags/caption.html#caption","text":"IM2P","title":"caption"},{"location":"datamaestro/tags/paragraphs.html","text":"paragraphs IM2P","title":"paragraphs"},{"location":"datamaestro/tags/paragraphs.html#paragraphs","text":"IM2P","title":"paragraphs"},{"location":"datamaestro/tags/word embeddings.html","text":"word embeddings GloVe word embeddings","title":"word embeddings"},{"location":"datamaestro/tags/word embeddings.html#word-embeddings","text":"GloVe word embeddings","title":"word embeddings"},{"location":"datamaestro/tasks.html","text":"List of Tasks collaborative filtering dependency parsing information extraction language model language modeling POS parsing Question Answering recommendation Visual Question Answering","title":"List of tasks"},{"location":"datamaestro/tasks.html#list-of-tasks","text":"collaborative filtering dependency parsing information extraction language model language modeling POS parsing Question Answering recommendation Visual Question Answering","title":"List of Tasks"},{"location":"datamaestro/tasks/recommendation.html","text":"recommendation GoodBooks-10k MovieLens 20M","title":"recommendation"},{"location":"datamaestro/tasks/recommendation.html#recommendation","text":"GoodBooks-10k MovieLens 20M","title":"recommendation"},{"location":"datamaestro/tasks/collaborative filtering.html","text":"collaborative filtering GoodBooks-10k MovieLens 20M","title":"collaborative filtering"},{"location":"datamaestro/tasks/collaborative filtering.html#collaborative-filtering","text":"GoodBooks-10k MovieLens 20M","title":"collaborative filtering"},{"location":"datamaestro/tasks/question answering.html","text":"Question Answering WikiQA bAbI SQuAD QUAC Visual Question Answering v2","title":"Question Answering"},{"location":"datamaestro/tasks/question answering.html#question-answering","text":"WikiQA bAbI SQuAD QUAC Visual Question Answering v2","title":"Question Answering"},{"location":"datamaestro/tasks/language model.html","text":"language model BookCorpus","title":"language model"},{"location":"datamaestro/tasks/language model.html#language-model","text":"BookCorpus","title":"language model"},{"location":"datamaestro/tasks/language modeling.html","text":"language modeling Wiki-2 Wiki-103","title":"language modeling"},{"location":"datamaestro/tasks/language modeling.html#language-modeling","text":"Wiki-2 Wiki-103","title":"language modeling"},{"location":"datamaestro/tasks/visual question answering.html","text":"Visual Question Answering Visual Question Answering v2","title":"Visual Question Answering"},{"location":"datamaestro/tasks/visual question answering.html#visual-question-answering","text":"Visual Question Answering v2","title":"Visual Question Answering"},{"location":"datamaestro/tasks/pos parsing.html","text":"POS parsing UD_French-ParTUT UD_French-ParTUT","title":"POS parsing"},{"location":"datamaestro/tasks/pos parsing.html#pos-parsing","text":"UD_French-ParTUT UD_French-ParTUT","title":"POS parsing"},{"location":"datamaestro/tasks/dependency parsing.html","text":"dependency parsing UD_French-ParTUT UD_French-ParTUT","title":"dependency parsing"},{"location":"datamaestro/tasks/dependency parsing.html#dependency-parsing","text":"UD_French-ParTUT UD_French-ParTUT","title":"dependency parsing"},{"location":"datamaestro/tasks/information extraction.html","text":"information extraction FewRel","title":"information extraction"},{"location":"datamaestro/tasks/information extraction.html#information-extraction","text":"FewRel","title":"information extraction"},{"location":"datamaestro/df/text/gov.nist.trec.index.html","text":"gov.nist.trec.index The Text REtrieval Conference (TREC), co-sponsored by the National Institute of Standards and Technology (NIST) and U.S. Department of Defense, was started in 1992 as part of the TIPSTER Text program. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In particular, the TREC workshop series has the following goals: to encourage research in information retrieval based on large test collections; to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provides a test set of documents and questions. Participants run their own retrieval systems on the data, and return to NIST a list of the retrieved top-ranked documents. NIST pools the individual results, judges the retrieved documents for correctness, and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. This evaluation effort has grown in both the number of participating systems and the number of tasks each year. Ninety-three groups representing 22 countries participated in TREC 2003. The TREC test collections and evaluation software are available to the retrieval research community at large, so organizations can evaluate their own retrieval systems at any time. TREC has successfully met its dual goals of improving the state-of-the-art in information retrieval and of facilitating technology transfer. Retrieval system effectiveness approximately doubled in the first six years of TREC. TREC has also sponsored the first large-scale evaluations of the retrieval of non-English (Spanish and Chinese) documents, retrieval of recordings of speech, and retrieval across multiple languages. TREC has also introduced evaluations for open-domain question answering and content-based retrieval of digital video. The TREC test collections are large enough so that they realistically model operational settings. Most of today's commercial search engines include technology first developed in TREC.","title":"gov.nist.trec.index"},{"location":"datamaestro/df/text/gov.nist.trec.index.html#govnisttrecindex","text":"The Text REtrieval Conference (TREC), co-sponsored by the National Institute of Standards and Technology (NIST) and U.S. Department of Defense, was started in 1992 as part of the TIPSTER Text program. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In particular, the TREC workshop series has the following goals: to encourage research in information retrieval based on large test collections; to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provides a test set of documents and questions. Participants run their own retrieval systems on the data, and return to NIST a list of the retrieved top-ranked documents. NIST pools the individual results, judges the retrieved documents for correctness, and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. This evaluation effort has grown in both the number of participating systems and the number of tasks each year. Ninety-three groups representing 22 countries participated in TREC 2003. The TREC test collections and evaluation software are available to the retrieval research community at large, so organizations can evaluate their own retrieval systems at any time. TREC has successfully met its dual goals of improving the state-of-the-art in information retrieval and of facilitating technology transfer. Retrieval system effectiveness approximately doubled in the first six years of TREC. TREC has also sponsored the first large-scale evaluations of the retrieval of non-English (Spanish and Chinese) documents, retrieval of recordings of speech, and retrieval across multiple languages. TREC has also introduced evaluations for open-domain question answering and content-based retrieval of digital video. The TREC test collections are large enough so that they realistically model operational settings. Most of today's commercial search engines include technology first developed in TREC.","title":"gov.nist.trec.index"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html","text":"TREC adhoc datasets TREC adhoc datasets List of datasets TREC adhoc datasets TREC-1 to TREC-3 documents TREC 1 topics TREC 1 relevance assessments Ad-hoc task of TREC 1 (1992) TREC 2 topics TREC 2 relevance assessments Ad-hoc task of TREC 2 (1993) TREC 3 topics TREC 3 relevance assessments Ad-hoc task of TREC 3 (1994) Data collection used in TREC-4 TREC 4 topics TREC 4 relevance assessments Ad-hoc task of TREC 4 (1995) Data collection used in TREC-5 TREC 5 topics TREC 5 relevance assessments Ad-hoc task of TREC 5 (1996) Data collection used in TREC-6 TREC 6 topics TREC 6 relevance assessments Ad-hoc task of TREC 6 (1997) Data collection used in TREC-7 and TREC-8 TREC 7 topics TREC 7 relevance assessments Ad-hoc task of TREC 3 (1994) TREC 8 topics TREC 8 relevance assessments Ad-hoc task of TREC 8 (1999) TREC Robust 2004 topics TREC Robust 2004 relevance assessments Ad-hoc task of TREC Robust (2004) TREC Robust 2005 topics TREC Robust 2005 relevance assessments Ad-hoc task of TREC Robust (2005)","title":"TREC adhoc datasets"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-adhoc-datasets","text":"TREC adhoc datasets","title":"TREC adhoc datasets"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-adhoc-datasets_1","text":"","title":"TREC adhoc datasets"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-1-to-trec-3-documents","text":"","title":"TREC-1 to TREC-3 documents"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-1-topics","text":"","title":"TREC 1 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-1-relevance-assessments","text":"","title":"TREC 1 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-1-1992","text":"","title":"Ad-hoc task of TREC 1 (1992)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-2-topics","text":"","title":"TREC 2 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-2-relevance-assessments","text":"","title":"TREC 2 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-2-1993","text":"","title":"Ad-hoc task of TREC 2 (1993)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-3-topics","text":"","title":"TREC 3 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-3-relevance-assessments","text":"","title":"TREC 3 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-3-1994","text":"","title":"Ad-hoc task of TREC 3 (1994)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#data-collection-used-in-trec-4","text":"","title":"Data collection used in TREC-4"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-4-topics","text":"","title":"TREC 4 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-4-relevance-assessments","text":"","title":"TREC 4 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-4-1995","text":"","title":"Ad-hoc task of TREC 4 (1995)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#data-collection-used-in-trec-5","text":"","title":"Data collection used in TREC-5"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-5-topics","text":"","title":"TREC 5 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-5-relevance-assessments","text":"","title":"TREC 5 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-5-1996","text":"","title":"Ad-hoc task of TREC 5 (1996)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#data-collection-used-in-trec-6","text":"","title":"Data collection used in TREC-6"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-6-topics","text":"","title":"TREC 6 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-6-relevance-assessments","text":"","title":"TREC 6 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-6-1997","text":"","title":"Ad-hoc task of TREC 6 (1997)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#data-collection-used-in-trec-7-and-trec-8","text":"","title":"Data collection used in TREC-7 and TREC-8"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-7-topics","text":"","title":"TREC 7 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-7-relevance-assessments","text":"","title":"TREC 7 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-3-1994_1","text":"","title":"Ad-hoc task of TREC 3 (1994)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-8-topics","text":"","title":"TREC 8 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-8-relevance-assessments","text":"","title":"TREC 8 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-8-1999","text":"","title":"Ad-hoc task of TREC 8 (1999)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-robust-2004-topics","text":"","title":"TREC Robust 2004 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-robust-2004-relevance-assessments","text":"","title":"TREC Robust 2004 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-robust-2004","text":"","title":"Ad-hoc task of TREC Robust (2004)"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-robust-2005-topics","text":"","title":"TREC Robust 2005 topics"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#trec-robust-2005-relevance-assessments","text":"","title":"TREC Robust 2005 relevance assessments"},{"location":"datamaestro/df/text/gov.nist.trec.adhoc.html#ad-hoc-task-of-trec-robust-2005","text":"","title":"Ad-hoc task of TREC Robust (2005)"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html","text":"gov.nist.trec.clueweb List of datasets gov.nist.trec.clueweb Tags : documents, text, html, web gov.nist.trec.clueweb.09 gov.nist.trec.clueweb.09b gov.nist.trec.clueweb.12 gov.nist.trec.clueweb.12b","title":"gov.nist.trec.clueweb"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb","text":"","title":"gov.nist.trec.clueweb"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb_1","text":"Tags : documents, text, html, web","title":"gov.nist.trec.clueweb"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb09","text":"","title":"gov.nist.trec.clueweb.09"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb09b","text":"","title":"gov.nist.trec.clueweb.09b"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb12","text":"","title":"gov.nist.trec.clueweb.12"},{"location":"datamaestro/df/text/gov.nist.trec.clueweb.html#govnisttrecclueweb12b","text":"","title":"gov.nist.trec.clueweb.12b"},{"location":"datamaestro/df/text/gov.nist.trec.web.html","text":"gov.nist.trec.web Tags : documents, text, html, web","title":"gov.nist.trec.web"},{"location":"datamaestro/df/text/gov.nist.trec.web.html#govnisttrecweb","text":"Tags : documents, text, html, web","title":"gov.nist.trec.web"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html","text":"gov.nist.trec.tipster TIPSTER is sometimes also called the Text Research Collection Volume or TREC. The TIPSTER project was sponsored by the Software and Intelligent Systems Technology Office of the Advanced Research Projects Agency (ARPA/SISTO) in an effort to significantly advance the state of the art in effective document detection (information retrieval) and data extraction from large, real-world data collections. The detection data is comprised of a test collection built at NIST for the TIPSTER project and the related TREC project. The TREC project has many other participating information retrieval research groups, working on the same task as the TIPSTER groups, but meeting once a year in a workshop to compare results (similar to MUC). The test collection consists of three CD-ROMs of SGML encoded documents distributed by LDC plus queries and answers (relevant documents) distributed by NIST. List of datasets gov.nist.trec.tipster gov.nist.trec.tipster.ap88 gov.nist.trec.tipster.ap89 gov.nist.trec.tipster.ap90 gov.nist.trec.tipster.doe1 gov.nist.trec.tipster.wsj87 gov.nist.trec.tipster.wsj88 gov.nist.trec.tipster.wsj89 gov.nist.trec.tipster.wsj90 gov.nist.trec.tipster.wsj91 gov.nist.trec.tipster.wsj92 gov.nist.trec.tipster.fr88 gov.nist.trec.tipster.fr89 gov.nist.trec.tipster.fr94 gov.nist.trec.tipster.ziff1 gov.nist.trec.tipster.ziff2 gov.nist.trec.tipster.ziff3 gov.nist.trec.tipster.sjm1 gov.nist.trec.tipster.cr1 gov.nist.trec.tipster.ft1 gov.nist.trec.tipster.fbis1 gov.nist.trec.tipster.la8990","title":"gov.nist.trec.tipster"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipster","text":"TIPSTER is sometimes also called the Text Research Collection Volume or TREC. The TIPSTER project was sponsored by the Software and Intelligent Systems Technology Office of the Advanced Research Projects Agency (ARPA/SISTO) in an effort to significantly advance the state of the art in effective document detection (information retrieval) and data extraction from large, real-world data collections. The detection data is comprised of a test collection built at NIST for the TIPSTER project and the related TREC project. The TREC project has many other participating information retrieval research groups, working on the same task as the TIPSTER groups, but meeting once a year in a workshop to compare results (similar to MUC). The test collection consists of three CD-ROMs of SGML encoded documents distributed by LDC plus queries and answers (relevant documents) distributed by NIST.","title":"gov.nist.trec.tipster"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipster_1","text":"","title":"gov.nist.trec.tipster"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterap88","text":"","title":"gov.nist.trec.tipster.ap88"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterap89","text":"","title":"gov.nist.trec.tipster.ap89"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterap90","text":"","title":"gov.nist.trec.tipster.ap90"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterdoe1","text":"","title":"gov.nist.trec.tipster.doe1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj87","text":"","title":"gov.nist.trec.tipster.wsj87"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj88","text":"","title":"gov.nist.trec.tipster.wsj88"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj89","text":"","title":"gov.nist.trec.tipster.wsj89"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj90","text":"","title":"gov.nist.trec.tipster.wsj90"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj91","text":"","title":"gov.nist.trec.tipster.wsj91"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterwsj92","text":"","title":"gov.nist.trec.tipster.wsj92"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterfr88","text":"","title":"gov.nist.trec.tipster.fr88"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterfr89","text":"","title":"gov.nist.trec.tipster.fr89"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterfr94","text":"","title":"gov.nist.trec.tipster.fr94"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterziff1","text":"","title":"gov.nist.trec.tipster.ziff1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterziff2","text":"","title":"gov.nist.trec.tipster.ziff2"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterziff3","text":"","title":"gov.nist.trec.tipster.ziff3"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipstersjm1","text":"","title":"gov.nist.trec.tipster.sjm1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipstercr1","text":"","title":"gov.nist.trec.tipster.cr1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterft1","text":"","title":"gov.nist.trec.tipster.ft1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterfbis1","text":"","title":"gov.nist.trec.tipster.fbis1"},{"location":"datamaestro/df/text/gov.nist.trec.tipster.html#govnisttrectipsterla8990","text":"","title":"gov.nist.trec.tipster.la8990"},{"location":"datamaestro/df/text/com.sentiment140.html","text":"Sentiment140 The data is a CSV with emoticons removed. Data file format has 6 fields: 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) 1 - the id of the tweet (2087) 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009) 3 - the query (lyx). If there is no query, then this value is NO_QUERY. 4 - the user that tweeted (robotickilldozr) 5 - the text of the tweet (Lyx is cool) If you use this data, please cite Sentiment140 as your source. How was your data collected and annotated? Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search. This is described in our paper. Where is the tweet corpus for Spanish?","title":"Sentiment140"},{"location":"datamaestro/df/text/com.sentiment140.html#sentiment140","text":"The data is a CSV with emoticons removed. Data file format has 6 fields: 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) 1 - the id of the tweet (2087) 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009) 3 - the query (lyx). If there is no query, then this value is NO_QUERY. 4 - the user that tweeted (robotickilldozr) 5 - the text of the tweet (Lyx is cool) If you use this data, please cite Sentiment140 as your source. How was your data collected and annotated? Our approach was unique because our training data was automatically created, as opposed to having humans manual annotate tweets. In our approach, we assume that any tweet with positive emoticons, like :), were positive, and tweets with negative emoticons, like :(, were negative. We used the Twitter Search API to collect these tweets by using keyword search. This is described in our paper. Where is the tweet corpus for Spanish?","title":"Sentiment140"},{"location":"datamaestro/df/text/com.fastml.goodbooks-10k.html","text":"GoodBooks-10k The dataset contains six million ratings for ten thousand most popular books (with most ratings). explicit ratings implicit feedback indicators (books marked to read) tabular data (book info) tags Tasks : recommendation, collaborative filtering","title":"GoodBooks-10k"},{"location":"datamaestro/df/text/com.fastml.goodbooks-10k.html#goodbooks-10k","text":"The dataset contains six million ratings for ten thousand most popular books (with most ratings). explicit ratings implicit feedback indicators (books marked to read) tabular data (book info) tags Tasks : recommendation, collaborative filtering","title":"GoodBooks-10k"},{"location":"datamaestro/df/text/com.microsoft.wikiqa.html","text":"WikiQA The WikiQA corpus is a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Tasks : Question Answering","title":"WikiQA"},{"location":"datamaestro/df/text/com.microsoft.wikiqa.html#wikiqa","text":"The WikiQA corpus is a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Tasks : Question Answering","title":"WikiQA"},{"location":"datamaestro/df/text/com.facebook.babi.html","text":"bAbI This page gather resources related to the bAbI project of Facebook AI Research which is organized towards the goal of automatic text understanding and reasoning. The datasets we have released consist of: The (20) QA bAbI tasks The (6) dialog bAbI tasks The Children\u2019s Book Test The Movie Dialog dataset The WikiMovies dataset The Dialog-based Language Learning dataset The SimpleQuestions dataset HITL Dialogue Simulator List of datasets bAbI Tags : synthetic Tasks : question answering None","title":"bAbI"},{"location":"datamaestro/df/text/com.facebook.babi.html#babi","text":"This page gather resources related to the bAbI project of Facebook AI Research which is organized towards the goal of automatic text understanding and reasoning. The datasets we have released consist of: The (20) QA bAbI tasks The (6) dialog bAbI tasks The Children\u2019s Book Test The Movie Dialog dataset The WikiMovies dataset The Dialog-based Language Learning dataset The SimpleQuestions dataset HITL Dialogue Simulator","title":"bAbI"},{"location":"datamaestro/df/text/com.facebook.babi.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/com.facebook.babi.html#babi_1","text":"Tags : synthetic Tasks : question answering","title":"bAbI"},{"location":"datamaestro/df/text/com.facebook.babi.html#none","text":"","title":"None"},{"location":"datamaestro/df/text/com.github.soskek.bookcorpus.html","text":"BookCorpus Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This work aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for. Tags : text, novels Tasks : language model","title":"BookCorpus"},{"location":"datamaestro/df/text/com.github.soskek.bookcorpus.html#bookcorpus","text":"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This work aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for. Tags : text, novels Tasks : language model","title":"BookCorpus"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html","text":"Wiki-2 The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies. List of datasets Wiki-2 Tags : text Tasks : language modeling Wiki-2 Wiki-2","title":"Wiki-2"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html#wiki-2","text":"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.","title":"Wiki-2"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html#wiki-2_1","text":"Tags : text Tasks : language modeling","title":"Wiki-2"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html#wiki-2_2","text":"","title":"Wiki-2"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-2.html#wiki-2_3","text":"","title":"Wiki-2"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html","text":"Wiki-103 The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies. List of datasets Wiki-103 Tags : text Tasks : language modeling Wiki-103 Wiki-103","title":"Wiki-103"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html#wiki-103","text":"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.","title":"Wiki-103"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html#wiki-103_1","text":"Tags : text Tasks : language modeling","title":"Wiki-103"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html#wiki-103_2","text":"","title":"Wiki-103"},{"location":"datamaestro/df/text/io.metamind.research.wikitext-103.html#wiki-103_3","text":"","title":"Wiki-103"},{"location":"datamaestro/df/text/io.github.rajpurkar.squad.html","text":"SQuAD Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. List of datasets SQuAD Tags : language comprehension, natural language inference, question anwering Tasks : Question Answering SQuAD","title":"SQuAD"},{"location":"datamaestro/df/text/io.github.rajpurkar.squad.html#squad","text":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","title":"SQuAD"},{"location":"datamaestro/df/text/io.github.rajpurkar.squad.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/io.github.rajpurkar.squad.html#squad_1","text":"Tags : language comprehension, natural language inference, question anwering Tasks : Question Answering","title":"SQuAD"},{"location":"datamaestro/df/text/io.github.rajpurkar.squad.html#squad_2","text":"","title":"SQuAD"},{"location":"datamaestro/df/text/ai.quac.html","text":"QUAC Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context. Tags : Context, Dialogue Tasks : Question Answering","title":"QUAC"},{"location":"datamaestro/df/text/ai.quac.html#quac","text":"Question Answering in Context is a dataset for modeling, understanding, and participating in information seeking dialog. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context. Tags : Context, Dialogue Tasks : Question Answering","title":"QUAC"},{"location":"datamaestro/df/text/org.acm.recsys.cb2014.html","text":"CBRecSys 2014 The corpus contains 1,832,307 xml files, each with combined book metadata from Amazon and LibraryThing. The filenames correspond to the ID number of each book on LibraryThing. Note that some works can have multiple editions, therefore a single book description can have multiple book titles, creators, etc. Each description has the following complex elements: - editions (edition information: ISBN, title, creators, publisher, etc) - similarproducts (ISBNs of books bought by the same customers) - browseNodes (Amazon categories) - subjects (library subject headings) - reviews (Amazon user reviews with ratings) - tags (LibraryThing user tags) The whole corpus is available as a single gzipped tarball file. The file unpacks into a single directory, with subdirectories corresponding to the last three digits of the IDs. Thus, all IDs ending in 352 are stored in the 352/ subdirectory. This corpus is derived from the INEX Amazon/LibraryThing corpus made for the INEX Interactive Track and Social Book Search Track. Tags : collaborative filtering, recommendation","title":"CBRecSys 2014"},{"location":"datamaestro/df/text/org.acm.recsys.cb2014.html#cbrecsys-2014","text":"The corpus contains 1,832,307 xml files, each with combined book metadata from Amazon and LibraryThing. The filenames correspond to the ID number of each book on LibraryThing. Note that some works can have multiple editions, therefore a single book description can have multiple book titles, creators, etc. Each description has the following complex elements: - editions (edition information: ISBN, title, creators, publisher, etc) - similarproducts (ISBNs of books bought by the same customers) - browseNodes (Amazon categories) - subjects (library subject headings) - reviews (Amazon user reviews with ratings) - tags (LibraryThing user tags) The whole corpus is available as a single gzipped tarball file. The file unpacks into a single directory, with subdirectories corresponding to the last three digits of the IDs. Thus, all IDs ending in 352 are stored in the 352/ subdirectory. This corpus is derived from the INEX Amazon/LibraryThing corpus made for the INEX Interactive Track and Social Book Search Track. Tags : collaborative filtering, recommendation","title":"CBRecSys 2014"},{"location":"datamaestro/df/text/org.grouplens.movielens20m.html","text":"MovieLens 20M Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags. List of datasets MovieLens 20M Tasks : Recommendation, Collaborative Filtering MovieLens 20M","title":"MovieLens 20M"},{"location":"datamaestro/df/text/org.grouplens.movielens20m.html#movielens-20m","text":"Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.","title":"MovieLens 20M"},{"location":"datamaestro/df/text/org.grouplens.movielens20m.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/org.grouplens.movielens20m.html#movielens-20m_1","text":"Tasks : Recommendation, Collaborative Filtering","title":"MovieLens 20M"},{"location":"datamaestro/df/text/org.grouplens.movielens20m.html#movielens-20m_2","text":"","title":"MovieLens 20M"},{"location":"datamaestro/df/text/org.pytorch.surnames.html","text":"Surname datasets Included in the data/names directory are 18 text files named as \u201c[Language].txt\u201d. Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).","title":"Surname datasets"},{"location":"datamaestro/df/text/org.pytorch.surnames.html#surname-datasets","text":"Included in the data/names directory are 18 text files named as \u201c[Language].txt\u201d. Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).","title":"Surname datasets"},{"location":"datamaestro/df/text/org.visualqa.balanced.html","text":"Visual Question Answering v2 List of datasets Visual Question Answering v2 Tasks : Question Answering, Visual Question Answering Balanced Real Images Balanced Abstract Images","title":"Visual Question Answering v2"},{"location":"datamaestro/df/text/org.visualqa.balanced.html#visual-question-answering-v2","text":"","title":"Visual Question Answering v2"},{"location":"datamaestro/df/text/org.visualqa.balanced.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/org.visualqa.balanced.html#visual-question-answering-v2_1","text":"Tasks : Question Answering, Visual Question Answering","title":"Visual Question Answering v2"},{"location":"datamaestro/df/text/org.visualqa.balanced.html#balanced-real-images","text":"","title":"Balanced Real Images"},{"location":"datamaestro/df/text/org.visualqa.balanced.html#balanced-abstract-images","text":"","title":"Balanced Abstract Images"},{"location":"datamaestro/df/text/org.cocodataset.index.html","text":"COmmon Objects in Context COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoints Tags : image, segmented images, object recognition","title":"COmmon Objects in Context"},{"location":"datamaestro/df/text/org.cocodataset.index.html#common-objects-in-context","text":"COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features: Object segmentation Recognition in context Superpixel stuff segmentation 330K images (>200K labeled) 1.5 million object instances 80 object categories 91 stuff categories 5 captions per image 250,000 people with keypoints Tags : image, segmented images, object recognition","title":"COmmon Objects in Context"},{"location":"datamaestro/df/text/org.universaldependencies.french.gsd.html","text":"UD_French-ParTUT The UD_French-GSD was converted in 2015 from the content head version of the universal dependency treebank v2.0 (https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 independently from the previous source. Tags : french Tasks : POS parsing, dependency parsing","title":"UD_French-ParTUT"},{"location":"datamaestro/df/text/org.universaldependencies.french.gsd.html#ud_french-partut","text":"The UD_French-GSD was converted in 2015 from the content head version of the universal dependency treebank v2.0 (https://github.com/ryanmcd/uni-dep-tb). It is updated since 2015 independently from the previous source. Tags : french Tasks : POS parsing, dependency parsing","title":"UD_French-ParTUT"},{"location":"datamaestro/df/text/org.universaldependencies.french.partut.html","text":"UD_French-ParTUT UD_French-ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts and Wikipedia articles, among others. Tags : french Tasks : POS parsing, dependency parsing","title":"UD_French-ParTUT"},{"location":"datamaestro/df/text/org.universaldependencies.french.partut.html#ud_french-partut","text":"UD_French-ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety of text genres, including talks, legal texts and Wikipedia articles, among others. Tags : french Tasks : POS parsing, dependency parsing","title":"UD_French-ParTUT"},{"location":"datamaestro/df/text/uk.ac.ucl.cs.qangaroo.html","text":"QAngaroo We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. Several pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps. Our aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents. The two QAngaroo datasets provide a training and evaluation resource for such methods. Tags : language comprehension, natural language inference, multi-hop","title":"QAngaroo"},{"location":"datamaestro/df/text/uk.ac.ucl.cs.qangaroo.html#qangaroo","text":"We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. Several pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps. Our aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents. The two QAngaroo datasets provide a training and evaluation resource for such methods. Tags : language comprehension, natural language inference, multi-hop","title":"QAngaroo"},{"location":"datamaestro/df/text/edu.upenn.ldc.nyt.html","text":"edu.upenn.ldc.nyt","title":"edu.upenn.ldc.nyt"},{"location":"datamaestro/df/text/edu.upenn.ldc.nyt.html#eduupennldcnyt","text":"","title":"edu.upenn.ldc.nyt"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html","text":"AQUAINT The AQUAINT Corpus, Linguistic Data Consortium (LDC) catalog number LDC2002T31 and ISBN 1-58563-240-6 consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST). List of datasets AQUAINT AQUAINT AQUAINT AQUAINT","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#aquaint","text":"The AQUAINT Corpus, Linguistic Data Consortium (LDC) catalog number LDC2002T31 and ISBN 1-58563-240-6 consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#aquaint_1","text":"","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#aquaint_2","text":"","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#aquaint_3","text":"","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.upenn.ldc.aquaint.html#aquaint_4","text":"","title":"AQUAINT"},{"location":"datamaestro/df/text/edu.cornell.nlvr.html","text":"Cornell Natural Language Visual Reasoning Cornell Natural Language Visual Reasoning (NLVR) is a language grounding dataset. It contains 92,244 pairs of natural language statements grounded in synthetic images. The task is to determine whether a sentence is true or false about an image. The data was collected through crowdsourcing, and requires reasoning about sets of objects, quantities, comparisons, and spatial relations. Tags : visual reasoning, visual inference","title":"Cornell Natural Language Visual Reasoning"},{"location":"datamaestro/df/text/edu.cornell.nlvr.html#cornell-natural-language-visual-reasoning","text":"Cornell Natural Language Visual Reasoning (NLVR) is a language grounding dataset. It contains 92,244 pairs of natural language statements grounded in synthetic images. The task is to determine whether a sentence is true or false about an image. The data was collected through crowdsourcing, and requires reasoning about sets of objects, quantities, comparisons, and spatial relations. Tags : visual reasoning, visual inference","title":"Cornell Natural Language Visual Reasoning"},{"location":"datamaestro/df/text/edu.standford.aclimdb.html","text":"Large Movie Review Dataset","title":"Large Movie Review Dataset"},{"location":"datamaestro/df/text/edu.standford.aclimdb.html#large-movie-review-dataset","text":"","title":"Large Movie Review Dataset"},{"location":"datamaestro/df/text/edu.standford.im2p.html","text":"IM2P Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach. With this paper, we are releasing a new dataset to allow researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2487/2489 images. We show in our paper that the paragraphs are more diverse than their corresponding sentences descriptions with more verbs, co-references and adjectives. Since all the images are also part of the Visual Genome dataset, Each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs. ![https://cs.stanford.edu/people/ranjaykrishna/im2p/examples.png] Tags : image, caption, paragraphs","title":"IM2P"},{"location":"datamaestro/df/text/edu.standford.im2p.html#im2p","text":"Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach. With this paper, we are releasing a new dataset to allow researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2487/2489 images. We show in our paper that the paragraphs are more diverse than their corresponding sentences descriptions with more verbs, co-references and adjectives. Since all the images are also part of the Visual Genome dataset, Each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs. ![https://cs.stanford.edu/people/ranjaykrishna/im2p/examples.png] Tags : image, caption, paragraphs","title":"IM2P"},{"location":"datamaestro/df/text/edu.standford.glove.html","text":"GloVe word embeddings GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. List of datasets GloVe word embeddings Tags : word embeddings GloVe word embeddings GloVe word embeddings GloVe word embeddings GloVe word embeddings GloVe word embeddings GloVe word embeddings GloVe word embeddings","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings","text":"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_1","text":"Tags : word embeddings","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_2","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_3","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_4","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_5","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_6","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_7","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/edu.standford.glove.html#glove-word-embeddings_8","text":"","title":"GloVe word embeddings"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html","text":"Large Text Compression Benchmark The test data for the Large Text Compression Benchmark is the first 109 bytes of the English Wikipedia dump on Mar. 3, 2006. http://download.wikipedia.org/enwiki/20060303/enwiki-20060303-pages-articles.xml.bz2 (1.1 GB or 4.8 GB after decompressing with bzip2 - link no longer works). Results are also given for the first 108 bytes, which is also used for the Hutter Prize. The data is UTF-8 encoded XML consisting primarily of English text. enwik9 contains 243,426 article titles, of which 85,560 are links, and the rest are regular articles. The example fragment below shows a redirection of \"AdA\" to \"Ada programming language\" and the start of a regular article with title \"Anarchism\". The data is UTF-8 clean. All characters are in the range U'0000 to U'10FFFF with valid encodings of 1 to 4 bytes. The byte values 0xC0, 0xC1, and 0xF5-0xFF never occur. Also, in the Wikipedia dumps, there are no control characters in the range 0x00-0x1F except for 0x09 (tab) and 0x0A (linefeed). Linebreaks occur only on paragraph boundaries, so they always have a semantic purpose. In the example below, lines were broken at 80 characters, but in reality each paragraph is one long line. The data contains some URL encoded XHTML tags such as <ref&gt ... </ref> and <br /> which decode to ... (citation) and (line break). However, hypertext links have their own encoding. External links are enclosed in square brackets in the form [URL anchor text]. Internal links are encoded as [[Wikipedia title | anchor text]], omitting the title and vertical bar if the title and anchor text are identical. Non-English characters are sometimes URL encoded as &#945;, meaning \u03b1 (Greek alpha, \u03b1), but are more often coded directly as a UTF-8 byte sequence. List of datasets Large Text Compression Benchmark Tags : language comprehension, natural language inference, question anwering Large Text Compression Benchmark Large Text Compression Benchmark","title":"Large Text Compression Benchmark"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html#large-text-compression-benchmark","text":"The test data for the Large Text Compression Benchmark is the first 109 bytes of the English Wikipedia dump on Mar. 3, 2006. http://download.wikipedia.org/enwiki/20060303/enwiki-20060303-pages-articles.xml.bz2 (1.1 GB or 4.8 GB after decompressing with bzip2 - link no longer works). Results are also given for the first 108 bytes, which is also used for the Hutter Prize. The data is UTF-8 encoded XML consisting primarily of English text. enwik9 contains 243,426 article titles, of which 85,560 are links, and the rest are regular articles. The example fragment below shows a redirection of \"AdA\" to \"Ada programming language\" and the start of a regular article with title \"Anarchism\". The data is UTF-8 clean. All characters are in the range U'0000 to U'10FFFF with valid encodings of 1 to 4 bytes. The byte values 0xC0, 0xC1, and 0xF5-0xFF never occur. Also, in the Wikipedia dumps, there are no control characters in the range 0x00-0x1F except for 0x09 (tab) and 0x0A (linefeed). Linebreaks occur only on paragraph boundaries, so they always have a semantic purpose. In the example below, lines were broken at 80 characters, but in reality each paragraph is one long line. The data contains some URL encoded XHTML tags such as <ref&gt ... </ref> and <br /> which decode to ... (citation) and (line break). However, hypertext links have their own encoding. External links are enclosed in square brackets in the form [URL anchor text]. Internal links are encoded as [[Wikipedia title | anchor text]], omitting the title and vertical bar if the title and anchor text are identical. Non-English characters are sometimes URL encoded as &#945;, meaning \u03b1 (Greek alpha, \u03b1), but are more often coded directly as a UTF-8 byte sequence.","title":"Large Text Compression Benchmark"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html#list-of-datasets","text":"","title":"List of datasets"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html#large-text-compression-benchmark_1","text":"Tags : language comprehension, natural language inference, question anwering","title":"Large Text Compression Benchmark"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html#large-text-compression-benchmark_2","text":"","title":"Large Text Compression Benchmark"},{"location":"datamaestro/df/text/net.mattmahoney.enwiki.html#large-text-compression-benchmark_3","text":"","title":"Large Text Compression Benchmark"},{"location":"datamaestro/df/text/me.zhuhao.fewrel.html","text":"FewRel FewRel is a Few-shot Relation classification dataset, which features 70, 000 natural language sentences expressing 100 relations annotated by crowdworkers. Tasks : information extraction","title":"FewRel"},{"location":"datamaestro/df/text/me.zhuhao.fewrel.html#fewrel","text":"FewRel is a Few-shot Relation classification dataset, which features 70, 000 natural language sentences expressing 100 relations annotated by crowdworkers. Tasks : information extraction","title":"FewRel"}]}